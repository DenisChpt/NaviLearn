# NaviLearn

NaviLearn est un systÃ¨me d'apprentissage par renforcement pour la navigation collective adaptative. Il simule des agents autonomes qui apprennent Ã  naviguer dans des environnements complexes, Ã  collecter des ressources et Ã  collaborer entre eux.

![NaviLearn Demo](docs/images/navilearn_demo.png)

## ğŸŒŸ CaractÃ©ristiques

- **Apprentissage par renforcement avancÃ©**: ImplÃ©mentation des algorithmes DQN et PPO avec diverses amÃ©liorations
- **Navigation collective**: Les agents peuvent communiquer et collaborer pour atteindre des objectifs communs
- **Environnement dynamique**: Obstacles, ressources, conditions mÃ©tÃ©orologiques variables
- **Visualisation en temps rÃ©el**: ReprÃ©sentation graphique 2D/3D avec contrÃ´les interactifs
- **Architecture modulaire**: Facilement extensible pour ajouter de nouvelles fonctionnalitÃ©s

## ğŸ“‹ PrÃ©requis

- Python 3.8+
- PyTorch 1.8+
- Pygame 2.0+
- NumPy
- OpenGL (optionnel, pour le rendu 3D)

## ğŸ”§ Installation

1. Clonez le dÃ©pÃ´t:
   ```bash
   git clone https://github.com/votre-username/navilearn.git
   cd navilearn
   ```

2. Installez les dÃ©pendances:
   ```bash
   pip install -r requirements.txt
   ```

3. VÃ©rifiez l'installation:
   ```bash
   python main.py --config config.yaml --render
   ```

## ğŸš€ Utilisation

### Commandes de base

```bash
# Lancer une simulation avec visualisation
python main.py --config config.yaml --render

# Lancer une simulation sans visualisation (plus rapide pour l'entraÃ®nement)
python main.py --config config.yaml

# SpÃ©cifier un algorithme d'apprentissage particulier
python main.py --config config.yaml --algorithm ppo

# Charger un modÃ¨le prÃ©-entraÃ®nÃ©
python main.py --config config.yaml --load models/model_episode_50.pt

# DÃ©finir le niveau de journalisation
python main.py --config config.yaml --log-level DEBUG
```

### Configuration

NaviLearn utilise un fichier de configuration YAML pour dÃ©finir tous les paramÃ¨tres de la simulation. Vous pouvez personnaliser:

- La taille et la complexitÃ© de l'environnement
- Le nombre et les propriÃ©tÃ©s des agents
- Les paramÃ¨tres de l'algorithme d'apprentissage
- Les options de visualisation et de rendu

Exemple de configuration (dans `config.yaml`):

```yaml
environment:
  width: 1000
  height: 1000
  obstacleCount: 30
  resourceCount: 15
  weatherEnabled: true

agents:
  count: 10
  sensorRange: 150
  communicationRange: 200

reinforcement_learning:
  algorithm: "dqn"
  stateDimension: 44
  actionDimension: 9
  learningRate: 0.001
  gamma: 0.99
  prioritizedExperience: true
```

### ContrÃ´les de l'interface

Pendant la simulation avec visualisation, vous pouvez utiliser les contrÃ´les suivants:

- **Espace**: Pause/Reprise de la simulation
- **+/-**: Augmenter/Diminuer la vitesse de simulation
- **FlÃ¨ches Gauche/Droite**: Changer d'agent sÃ©lectionnÃ©
- **F**: Activer/DÃ©sactiver le suivi de l'agent sÃ©lectionnÃ©
- **G**: Activer/DÃ©sactiver la grille
- **P**: Activer/DÃ©sactiver l'affichage des trajectoires
- **S**: Activer/DÃ©sactiver l'affichage des capteurs
- **C**: Activer/DÃ©sactiver l'affichage des communications
- **R**: RÃ©initialiser la camÃ©ra
- **Souris**:
  - **Clic gauche**: SÃ©lection d'agent
  - **Clic droit + DÃ©placement**: Rotation de la camÃ©ra
  - **Clic milieu + DÃ©placement**: Panoramique
  - **Molette**: Zoom avant/arriÃ¨re

## ğŸ“Š Architecture du projet

```
navilearn/
â”œâ”€â”€ agents/                  # ImplÃ©mentation des agents
â”‚   â”œâ”€â”€ baseAgent.py         # Classe de base pour tous les agents
â”‚   â”œâ”€â”€ collectiveAgent.py   # Agent avec capacitÃ©s collaboratives
â”‚   â”œâ”€â”€ knowledgeMemory.py   # MÃ©moire de connaissances
â”‚   â””â”€â”€ smartNavigator.py    # Agent avec navigation intelligente
â”‚
â”œâ”€â”€ environment/             # ImplÃ©mentation de l'environnement
â”‚   â”œâ”€â”€ dynamicObstacle.py   # Obstacles statiques et dynamiques
â”‚   â”œâ”€â”€ resourceNode.py      # Ressources Ã  collecter
â”‚   â”œâ”€â”€ terrainGenerator.py  # GÃ©nÃ©ration procÃ©durale de terrain
â”‚   â”œâ”€â”€ weatherSystem.py     # SystÃ¨me mÃ©tÃ©orologique dynamique
â”‚   â””â”€â”€ world.py             # Environnement de simulation
â”‚
â”œâ”€â”€ rl/                      # Algorithmes d'apprentissage
â”‚   â”œâ”€â”€ dqnLearner.py        # ImplÃ©mentation de DQN
â”‚   â”œâ”€â”€ policyNetwork.py     # RÃ©seau de politique
â”‚   â”œâ”€â”€ ppoLearner.py        # ImplÃ©mentation de PPO
â”‚   â”œâ”€â”€ replayBuffer.py      # Tampons d'expÃ©rience
â”‚   â”œâ”€â”€ rewardSystem.py      # SystÃ¨me de rÃ©compense
â”‚   â””â”€â”€ valueNetwork.py      # RÃ©seau de valeur
â”‚
â”œâ”€â”€ utils/                   # Utilitaires divers
â”‚   â”œâ”€â”€ configManager.py     # Gestion de la configuration
â”‚   â”œâ”€â”€ logger.py            # Journalisation
â”‚   â”œâ”€â”€ mathHelper.py        # Fonctions mathÃ©matiques
â”‚   â””â”€â”€ profiler.py          # Analyse de performances
â”‚
â”œâ”€â”€ visualization/           # Visualisation et interface
â”‚   â”œâ”€â”€ animations.py        # SystÃ¨me d'animations
â”‚   â”œâ”€â”€ camera.py            # CamÃ©ra pour la visualisation 3D
â”‚   â”œâ”€â”€ guiManager.py        # Interface utilisateur
â”‚   â”œâ”€â”€ renderer.py          # Moteur de rendu
â”‚   â””â”€â”€ shaders.py           # Shaders OpenGL
â”‚
â”œâ”€â”€ config.yaml              # Configuration par dÃ©faut
â”œâ”€â”€ main.py                  # Point d'entrÃ©e principal
â”œâ”€â”€ requirements.txt         # DÃ©pendances du projet
â””â”€â”€ README.md                # Documentation
```

## ğŸ”¬ ExpÃ©rimentation

Pour conduire vos propres expÃ©riences, vous pouvez:

1. **Modifier les paramÃ¨tres d'apprentissage**:
   Ajustez les hyperparamÃ¨tres dans `config.yaml` pour explorer leur impact sur les performances.

2. **CrÃ©er de nouveaux environnements**:
   Modifiez les paramÃ¨tres de gÃ©nÃ©ration d'environnement pour crÃ©er des dÃ©fis plus complexes.

3. **ImplÃ©menter de nouveaux comportements d'agent**:
   Ã‰tendez la classe `CollectiveAgent` pour ajouter des comportements personnalisÃ©s.

4. **Comparer les algorithmes d'apprentissage**:
   Utilisez l'argument `--algorithm` pour comparer les performances de DQN et PPO.

## ğŸ” Cycle d'entraÃ®nement typique

1. **EntraÃ®nement initial**:
   ```bash
   python main.py --config config.yaml --episodes 100
   ```

2. **Ã‰valuation intermÃ©diaire**:
   ```bash
   python main.py --config config.yaml --load models/model_episode_100.pt --render
   ```

3. **EntraÃ®nement supplÃ©mentaire**:
   ```bash
   python main.py --config config.yaml --load models/model_episode_100.pt --episodes 100
   ```

4. **Visualisation finale**:
   ```bash
   python main.py --config config.yaml --load models/model_episode_200.pt --render
   ```

## ğŸ“ Analyse des rÃ©sultats

NaviLearn gÃ©nÃ¨re des statistiques dÃ©taillÃ©es pendant l'entraÃ®nement:
- Taux de collecte de ressources
- EfficacitÃ© de navigation
- Niveaux de collaboration entre agents
- Convergence de l'apprentissage

Ces statistiques sont affichÃ©es dans la visualisation et enregistrÃ©es dans les fichiers de journalisation pour une analyse ultÃ©rieure.

## ğŸ¤ Contribuer

Les contributions sont les bienvenues! Voici comment vous pouvez contribuer:

1. Forkez le projet
2. CrÃ©ez une branche pour votre fonctionnalitÃ© (`git checkout -b feature/amazing-feature`)
3. Committez vos changements (`git commit -m 'Add some amazing feature'`)
4. Poussez vers la branche (`git push origin feature/amazing-feature`)
5. Ouvrez une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ“® Contact

Pour toute question, suggestion ou commentaire, n'hÃ©sitez pas Ã  ouvrir une issue sur GitHub ou Ã  contacter les mainteneurs directement.

---

DÃ©veloppÃ© avec â¤ï¸ par l'Ã©quipe NaviLearn